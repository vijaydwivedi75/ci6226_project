{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI6226 Assignment\n",
    "\n",
    "## Team: GoogleX\n",
    "- Dwivedi Vijay Prakash (G1902961C)  \n",
    "- Shakya Manoj (G1902549)  \n",
    "- Emadeldeen Ahmed (G1900649D)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Toolkit Util Functions\n",
    "\n",
    "### Function for Directory Listing\n",
    "- Input: string (path to directory)  \n",
    "- Output: list of strings (full paths to files in the directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_files(path='HillaryEmails'):\n",
    "    root = join(os.getcwd(), path)\n",
    "    file_paths = [join(root, file) for file in os.listdir(path) if isfile(join(path, file))]\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for File Reading\n",
    "- Input: string (full path to file)  \n",
    "- Output: string/text (full contents of a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        content = content.replace(\"\\n\", \" \")     # removing \\n and replacing with space\n",
    "        content = content.replace(\"\\t\", \" \")     # removing \\t and replacing with space\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "### Fucntion for tokenization based on whitespace characters (space, newline, tab)\n",
    "- Input: text (file contents), string (document id = path to file)  \n",
    "- Output: list of pairs < string (token) , string (document id) >  \n",
    "\n",
    "The tokenize function defined below also takes care of multiple whitespace characters in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_content, file_path):\n",
    "    return [(x, file_path) for x in re.split(r\"([.,!?]+)?\\s+\", file_content) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('she', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('will', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('host', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('a', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D E M O    O F    T O K E N I Z E     F U N C T I O N\n",
    "\n",
    "file1 = list_all_files()[23]\n",
    "file_content = read_file(file1)\n",
    "\n",
    "tokenize(file_content, file1)[136:141]    # displaying only 5 for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linguistic Modules\n",
    "\n",
    "### Function for removing punctuation, lowercasing, and stemming\n",
    "- Input: list of pairs < token , document id >\n",
    "- Output: list of pairs < modified token , document id >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_clean(token_file_path_pairs):\n",
    "    modified_list = []\n",
    "    for pair in token_file_path_pairs:\n",
    "        token, file_path = pair\n",
    "\n",
    "        # Removing punctuations\n",
    "        punctuations = '''!@#$%^&*()-_=+'`~ \":;|/.,?[]{}<>'''\n",
    "        for char in token.lower(): \n",
    "            if char in punctuations: \n",
    "                token = token.replace(char, \"\") \n",
    "\n",
    "        # Lowercasing the token\n",
    "        token = token.lower()\n",
    "\n",
    "        # Stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        token = stemmer.stem(token)\n",
    "        \n",
    "        modified_list.append((token, file_path))\n",
    "    \n",
    "    return modified_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('she', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('will', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('host', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('a', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D E M O    O F    L I N G U I S T I C      C L E A N     F U N C T I O N\n",
    "\n",
    "file1 = list_all_files()[23]\n",
    "file_content = read_file(file1)\n",
    "\n",
    "token_docid_pairs = tokenize(file_content, file1)[136:141]    # displaying only 5 for demo\n",
    "\n",
    "linguistic_clean(token_docid_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sorting the Tokens\n",
    "\n",
    "### Function for sorting the tokens doc id pair, first by token, and then by doc id (file paths)\n",
    "- Input: list of pairs < token , document id >\n",
    "- Output: sorted list of pairs < token , document id >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tokens(token_file_path_pairs):\n",
    "    token_file_path_pairs.sort(key=lambda tup: tup[1])    # then by doc id (file paths)\n",
    "    token_file_path_pairs.sort(key=lambda tup: tup[0])    # sorting first by token\n",
    "    return token_file_path_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('host', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('she', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('that', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'),\n",
       " ('will', '/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D E M O    O F    S O R T     T O K E N S     F U N C T I O N\n",
    "\n",
    "linguistic_cleaned_token_docid_pairs = linguistic_clean(token_docid_pairs)\n",
    "sort_tokens(linguistic_cleaned_token_docid_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformation into Postings\n",
    "\n",
    "### Function for converting the sorted list of token doc id pairs to inverted index\n",
    "- Input: sorted list of pairs < token , document id >\n",
    "- Output: inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_posting(sorted_token_file_path_pairs):\n",
    "    inv_index, token_frequency = {}, {}\n",
    "    for pair in sorted_token_file_path_pairs:\n",
    "        token, file_path = pair\n",
    "        if not token in inv_index:\n",
    "            token_frequency[token] = 1\n",
    "            inv_index[token] = [file_path]\n",
    "        else:\n",
    "            if file_path != inv_index[token][-1]:\n",
    "                inv_index[token].append(file_path)\n",
    "                token_frequency[token] += 1\n",
    "    inv_index['__freq__'] = token_frequency\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ['/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'],\n",
       " 'host': ['/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'],\n",
       " 'she': ['/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'],\n",
       " 'that': ['/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'],\n",
       " 'will': ['/Users/Vijay/Desktop/IR_Project/HillaryEmails/4117.txt'],\n",
       " '__freq__': {'a': 1, 'host': 1, 'she': 1, 'that': 1, 'will': 1}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D E M O    O F    C R E A T E     P O S T I N G     F U N C T I O N\n",
    "\n",
    "sorted_token_file_path_pair = sort_tokens(linguistic_cleaned_token_docid_pairs)\n",
    "create_posting(sorted_token_file_path_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Postings List Merge\n",
    "\n",
    "### Function for merging posting lists of two or more tokens\n",
    "- Input: list of postings lists\n",
    "- Output: merged postings list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func_call(temp_dict)\n",
    "\n",
    "\n",
    "# key1\n",
    "# key2\n",
    "# key3\n",
    "\n",
    "# temp_dict = {}\n",
    "# temp_dict[key1] = inv_index[key1]\n",
    "# temp_dict[key2] = inv_index[key2]\n",
    "# temp_dict[key2] = inv_index[key3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D E M O      R U N\n",
    "\n",
    "# all_pairs = []\n",
    "# for file in list_all_files()[23:28]:\n",
    "#     text = read_file(file)\n",
    "#     all_pairs.extend(tokenize(text, file))\n",
    "\n",
    "# inv_index = create_posting(sort_tokens(linguistic_clean(all_pairs)))\n",
    "\n",
    "# inv_index['xpress']\n",
    "# inv_index['__freq__']['xpress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
